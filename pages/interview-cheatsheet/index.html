<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Interview Cheatsheet | Datatician</title><meta name=keywords content="career"><meta name=description content="Math/Statistics :PROPERTIES: :heading: true :collapsed: true :END:
Probability vs. Likelihood
Probability: During the testing phase, given a learned model, we determine the probability of observing the outcome Likelihood: During training, given some outcome we determine the likelihood of observing theta that maximizes the probability of that outcome (MLE). Bayes Theorem
Revise prediction using new evidence Naive Bayes Classifier: Generative Classification model Linear Transformation $f(\alpha x + \beta y)=\alpha f(x)+\beta f(y)$"><meta name=author content="Marco Christiani"><link rel=canonical href=https://datatician.io/pages/interview-cheatsheet/><link crossorigin=anonymous href=/assets/css/stylesheet.min.49b1c3611e5e823e974d0b9b3e1101617fce26c004757161abb1a1e4af3ff85c.css integrity="sha256-SbHDYR5egj6XTQubPhEBYX/OJsAEdXFhq7Gh5K8/+Fw=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://datatician.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://datatician.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://datatician.io/favicon-32x32.png><link rel=apple-touch-icon href=https://datatician.io/apple-touch-icon.png><link rel=mask-icon href=https://datatician.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.109.0"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Interview Cheatsheet"><meta property="og:description" content="Math/Statistics :PROPERTIES: :heading: true :collapsed: true :END:
Probability vs. Likelihood
Probability: During the testing phase, given a learned model, we determine the probability of observing the outcome Likelihood: During training, given some outcome we determine the likelihood of observing theta that maximizes the probability of that outcome (MLE). Bayes Theorem
Revise prediction using new evidence Naive Bayes Classifier: Generative Classification model Linear Transformation $f(\alpha x + \beta y)=\alpha f(x)+\beta f(y)$"><meta property="og:type" content="article"><meta property="og:url" content="https://datatician.io/pages/interview-cheatsheet/"><meta property="article:section" content="pages"><meta property="article:published_time" content="2023-01-03T00:00:00+00:00"><meta property="article:modified_time" content="2023-01-03T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Interview Cheatsheet"><meta name=twitter:description content="Math/Statistics :PROPERTIES: :heading: true :collapsed: true :END:
Probability vs. Likelihood
Probability: During the testing phase, given a learned model, we determine the probability of observing the outcome Likelihood: During training, given some outcome we determine the likelihood of observing theta that maximizes the probability of that outcome (MLE). Bayes Theorem
Revise prediction using new evidence Naive Bayes Classifier: Generative Classification model Linear Transformation $f(\alpha x + \beta y)=\alpha f(x)+\beta f(y)$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Pages","item":"https://datatician.io/pages/"},{"@type":"ListItem","position":3,"name":"Interview Cheatsheet","item":"https://datatician.io/pages/interview-cheatsheet/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Interview Cheatsheet","name":"Interview Cheatsheet","description":"Math/Statistics :PROPERTIES: :heading: true :collapsed: true :END:\nProbability vs. Likelihood\nProbability: During the testing phase, given a learned model, we determine the probability of observing the outcome Likelihood: During training, given some outcome we determine the likelihood of observing theta that maximizes the probability of that outcome (MLE). Bayes Theorem\nRevise prediction using new evidence Naive Bayes Classifier: Generative Classification model Linear Transformation $f(\\alpha x + \\beta y)=\\alpha f(x)+\\beta f(y)$","keywords":["career"],"articleBody":"Math/Statistics :PROPERTIES: :heading: true :collapsed: true :END:\nProbability vs. Likelihood\nProbability: During the testing phase, given a learned model, we determine the probability of observing the outcome Likelihood: During training, given some outcome we determine the likelihood of observing theta that maximizes the probability of that outcome (MLE). Bayes Theorem\nRevise prediction using new evidence Naive Bayes Classifier: Generative Classification model Linear Transformation $f(\\alpha x + \\beta y)=\\alpha f(x)+\\beta f(y)$\nLinear transformations: rotation (produced by shearing), scaling Affine Transformation $$f(\\alpha x + (1-\\alpha)y)=\\alpha f(x)+(1-\\alpha)f(y)$$ AKA a linear transformation plus translation $$f(x)=\\vec{a}^Tx+\\vec{b}$$ Properties Perserved:\nCollinearity between points: points on same line (collinear points) remain on same line after transformation Parallelism: Parallel lines remain parallel Convexity of sets: Furthermore, extreme points of original set map to extreme points of transformed set Length Ratios of Parallel lines Barycenters of weighted collections of points. Aka center of mass. ? Machine Learning :PROPERTIES: :heading: true :collapsed: true :END:\nDiscriminative Model\nlearn decision boundaries between classes SVM, Logistic Regression, Decision Trees Not great w outliers Maximizes conditional likelihood, given model parameters L(theta) = max P(y | x; theta) Generative Model\ndistribution of classes themselves Naïve Bayes, Discriminant Analysis (LDA, GDA) Better with outliers Maximizes joint likelihood: the joint probability given model parameters L(theta)=max P(x, y; theta) Cross Validation\nTypically use k-fold validation: i.e. leave one out cross validation Roll forward Cross Validation: used with Time Series Data Tree Pruning\nMitigate Overfitting Cost Effective Pruning:\nRemove a subtree (replacing with a leaf node) if resulting tree does not have a significant decrease in performance (delta formula) then keep the new pruned tree and repeat. Dealing with Datasets\nImbalanced Datasets\nRandom Under-sampling: Lots of data in smaller class Random Over-sampling: Not lots of data in smaller class Missing Data:\nImputation (i.e. 0), add a new category for categorical (I.e. “other”), interpolation Outliers\n*Analyze without and without outliers Trimming: Remove outliers Winsorizing: Ceil/Floor to a max/min non-outlier value SMOTE\nSynthetic Monetary Oversampling* Synthesize new data with minor noise added to existing sample rather than exact copies ROC :PROPERTIES: :id: 6304ff68-bb00-4200-aa44-975f5ef5aba9 :collapsed: true :END:\nReceiver Operator Characteristic Graphs Sensitivity vs Specificity (OR Precision) i.e. True Positive vs True Negative Rates [[../assets/unnamed_1661273383447_0.png]] Accuracy True predictions/Number points\nPrecision precision=TP/(TP+FP)\nHow many of our positive predictions were right? Positive Prediction Accuracy for the label Proportion of positive results that were correctly classified $\\text{precision}=\\text{true_pos}/(\\text{true_pos} + \\text{true_neg})$ Good if we have an imbalance such as way more negatives than positives (not in eq) Sensitivity/Recall \\begin{align} \\text{sensitivity}\u0026=TP/(TP+FN)\\ \u0026=\\text{true_pos}/(\\text{true_pos} + \\text{false_neg}) \\end{align}\nReturns the True Positive Rate of the label. Specificity: TN/(TN+FP) $$\\text{specificity}=\\text{false_pos}/(\\text{false_pos} + \\text{true_neg})$$\nAUC\nArea Under the Curve Used to compare ROC curves More AUC=better Neural Networks\nRNN\nHandle sequential data (unlike feedforward nn). Sentiment analysis, text mining, image captioning, time series problems CNNs\nImage matrix, filter matrix Slide filter matrix over the image acompute the dot product to get convolved feature matrix. CNN better than Dense NN for Images: Because less params (no overfit), more interpretable (can look at weights), CNNs can learn simple-to-complex patterns (learn complex patterns by learning several simple patterns) GANs\nUse a Generator and Discriminator (to build an accurate Discriminator model) Activation Functions\nSoftmax\nScales input to (0,1). Output layers ReLU\nClips input at 0, only non-negative outputs. Produces “rectified feature map.” Hidden layers Swish\nVariant of ReLU developed at google, better at some DL tasks Pooling\nPooling is a down-sampling operation that reduces the dimensionality of the feature map Computation Graph\nNodes are operations, Edges are tensors/data Batch Gradient vs Stochastic Gradient Descent\nAutoencoder :PROPERTIES: :collapsed: true :END:\n3 Layer model that tries to reconstruct its input using a hidden layer of fewer dimensions to create a latent space representation. In its most basic form, uses dimensionality reduction to perform filtering (i.e. noise). Regularized Autoencoders: Classification (include Sparse, Denoising, Contractive)\nVariational Autoencoders: Generative models\nUses\nExtract features and hidden patterns Dimensionality reduction Denoise im ages Convert black and w hite images into colored images. Transfer Learning\nModels: VGG-16, BERT, GPT-3, Inception, Xception Vanishing Gradients\nUse ReLU instead of tanh (try different activation function) Try Xavier initialization (takes into account number of inputs and outputs). ANN Hyperparameters\nBatch size: size of input data Epochs: number of times training data is visible to the neural network to train. Momentum: Dampen/attenuate oscillations in gradient descent. If the weights matrix is ill conditioned, this helps convergence speed up a bit. Learning rate: Represents the time required for the network to update the parameters and learn. Ensemble learning\n[[Boosting]], Bagging, Random Forest Aggregation mitigates overfitting of a class Bagging\nTrain several models and vote to produce output. Boosting\nUse a model to improve performance where another model is weakest. (i.e. model the error) Logistic Regression\nRegression for classification. Linear model produces logits, softmax(logits) produces prediction Fourier Transform\nDecompose functions into its constituent parts. Quant :PROPERTIES: :heading: true :id: 63050186-cb96-4e65-9445-a39fa91e2305 :collapsed: true :END:\nWhat factors in production could cause a backtested strategy to perform different than expected? Slippage, transaction costs, systemic risk, outside events that cannot be modeled such as state of global economy/climate/legislation/etc Black-Scholes\nOriginally to valuate European call options American equivalents: Bjerksund-Stendland model, binomial, trinomial models Uses 5 Factors: Volatility Price of underlying asset Strike price Time to expiration Risk free interest rate Black-Scholes Asumptions Price follows a random walk approximately Geometric brownian motion with constant drift and volatility (i.e. log(variance) is constant) No dividends over life of option Movements are random, market is random No transaction costs RFR and volatility are constant (not a strong assumption for volatility, since that is influenced by supply/demand) Returns are log normal Option is European (can only be exercised at expiration) Options\nOption Greeks\nDelta First derivative with respect to price. Rate of change of equilibrium price (aka BS price) with respect to asset price.\nGamma Second derivative with respect to price.\nTheta First derivative with respect to time-to-maturity. Rate of change of equilibrium price with respect to time-to-maturity.\nVega Rate of change of equilibrium price with respect to asset volatility.\nRho Rate of change of equilibrium price with respect to RF interest rate.\nCall Options\nBreak-even: K + P (where K is strike price and P is cost of option) 5 reasons to buy a call option Bet on upside move with minimal cost (lot of a exposure for little cost) Unlimited Upside Limited Downside: Can only lose what you paid for the option Increase in Volatility: Option is priced based on its volatility, so all we need is an increase in volatility to increase the value of our option Hedge Short Position: Unlimited upside offsets risk of short as shorts have unlimited downside Call-Spread\nMax Value: difference in strike prices. $v_{max} = K_2 - K_1$ Where $K_2=Sold$ and $K_1=bought$ strike prices Max Loss: $Loss_{max} = v_{max}-P_{cs}$ Max value - Price of call-spread Pay Off Diagrams\nPlot of Underlying Price vs. P\u0026L 3 Key Points: Maximum Loss Maximum Gain Break-even Point Put-Call Parity\nRepresents an arbitrage opportunity $\\text{call_price}+\\text{present_value_discounted} = \\text{put_price} + \\text{spot_price}$ (where present value is discounted from the value at RFR) Finance Fundamentals :PROPERTIES: :heading: true :id: 63050437-3d0c-448f-a280-7e3a230472ae :collapsed: true :END:\nFinancial Statements\nBalance sheet Shows a company’s…\nAssets Liabilities Shareholders’ equity (what it owns, owes, is worth) Highlights: liquidity, capital assets, credit metrics, liquidity ratios, leverage, ROA (return on assets), ROW (return on equity) Income Statement Shows a company’s…\nRevenue Expenses Net Income Highlights: Growth rates, margins, profitability Cash Flow Statement Shows a company’s cash inflows/outlflows from…\nOperation Investments Financing Highlights: short/long term cash flow profile, needs to raise money or return capital to shareholders What is the best financial statement to measure a company’s health?\nCash is king. /Cash Flow Statement/ shows how much cash company is actually generating Arguments for other statements: Balance Sheet: assets are true driver of cash flow Income Statement: Earning power and profitability on an accural basis WACC\nWeighted Average Cost of Capital Blended cost of capital across all sources (common/preferred shares, debt) (% debt vs total capital) x (1-effective tax rate)+(% equity vs capital) x (required return on equity) check this What is cheaper: debt or equity?\nDebt: backed by collateral and paid off before equity Debt is more liquid ? Finance Formulas :PROPERTIES: :heading: true :id: 63050473-e880-4c2c-863a-4b18153ab78d :collapsed: true :END:\n/Revenue/ = Volume x Price /Cost/ = Fixed Cost + Variable Cost /Profit/ = Revenue - Cost /Profitability or Profit Margin/ = Profit/Revenue /ROI/ = Annual Profit / Principal Investment /Breakeven or Payback Period/ = Principal / Annual Profit /ROE/ = Profits / Shareholder Equity /ROA/ = Profits / Total Assets Word Problems :PROPERTIES: :collapsed: true :END:\nGiven a random number generator which provides a random real value between 0 to 1, how can you estimate the value of pi? Monte Carlo integration (unit circle inside a square, ratio of points in circle versus points outside) Find the minimum number of socks I need to take out from a box of red and black socks to ensure that I have k pairs of socks. Use Pigeon Hole Principle Pick up N socks (one of each color) Next sock forms a pair Answer: 2k+N-1 Note: When coding ensure to check if K\u003etotal_pairs in list (pairs+=arr[i]/2) Can you minimize piecewise linear function without adding auxiliary variables? [[https://www.seas.ucla.edu/~vandenbe/ee236a/lectures/pwl.pdf][See this lecture]] Firstly: is the function convex Convex piecewise-linear (piecewise-affine is a more accurate term) can be expressed as: $$f(x)=\\max {i=1, \\ldots, m}\\left(a{i}^{T} x+b_{i}\\right)$$ Problem becomes: $\\min f(x)$ Therefore minimize each: $\\min t$ subject to $a_{i}^{T} x+b_{i} \\le t$ for $i=1,..,m$ Basically: no Code Puzzles :PROPERTIES: :heading: true :END:\nReverse a Linked List :PROPERTIES: :heading: true :collapsed: true :END: #+begin_src python Curr=head while curr: Next = curr.next Curr.next = prev Prev = curr Curr = next return prev (new head) #+end_src\nLongest Palindrome :PROPERTIES: :heading: true :collapsed: true :END: #+begin_src python def longest_palindrome(s: str): if not s: return “” longest = “” for i in range(len(s)): # odd case, like “aba” tmp = helper(s, i, i) if len(tmp) \u003e len(longest): # update result longest = tmp\n# even case, like “abba” tmp = helper(s, i, i+1) if len(tmp) \u003e len(longest): longest = tmp return longest\ndef helper(s: str, l: int, r: int): while l \u003e= 0 and r \u003c len(s) and s[l] == s[r]: l -= 1 #decrement the left r += 1 #increment the right return s[l+1:r]\n#+end_src\nBFS (not recursive) :PROPERTIES: :heading: true :collapsed: true :END: #+begin_src python\nVisit adjacent unvisited vertex. - Mark it as visited. Display it. Insert it in a queue. If no adjacent vertex, pop vertex off queue Repeat Rule 1 and Rule 2 until the queue is empty. def bfs(graph, current_node): visited = [] queue = [current_node]\nwhile queue: s = queue.pop(0) print(s) for neighbor in graph[s]: if neighbor not in visited: visited.append(neighbor) queue.append(neighbor) bfs(graph, ‘A’) #+end_src\nDFS (not recursive) :PROPERTIES: :heading: true :collapsed: true :END: #+begin_src python\nadd unvisited nodes to stack def dfs(graph, start_vertex): visited = set() traversal = [] stack = [start_vertex] while stack: vertex = stack.pop() if vertex not in visited: visited.add(vertex) traversal.append(vertex) stack.extend(reversed(graph[vertex])) # add in same order as visited return traversal\n#+end_src\nCheck if binary trees are equal: :PROPERTIES: :heading: true :END: #+begin_src python def are_identical(root1, root2): if root1 == None and root2 == None: return True\nif root1 != None and root2 != None: return (root1.data == root2.data and are_identical(root1.left, root2.left) and are_identical(root1.right, root2.right))\nreturn False #+end_src\nEtc :PROPERTIES: :heading: true :END: #+begin_src python #+end_src\n","wordCount":"1889","inLanguage":"en","datePublished":"2023-01-03T00:00:00Z","dateModified":"2023-01-03T00:00:00Z","author":{"@type":"Person","name":"Marco Christiani"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://datatician.io/pages/interview-cheatsheet/"},"publisher":{"@type":"Organization","name":"Datatician","logo":{"@type":"ImageObject","url":"https://datatician.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://datatician.io accesskey=h title="Datatician (Alt + H)">Datatician</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Interview Cheatsheet</h1><div class=post-meta><span title='2023-01-03 00:00:00 +0000 UTC'>January 3, 2023</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Marco Christiani</div></header><div class=post-content><p>Math/Statistics
:PROPERTIES:
:heading: true
:collapsed: true
:END:</p><p>Probability vs. Likelihood</p><ul><li><em>Probability</em>: During the testing phase, given a learned model, we determine the probability of observing the outcome</li><li><em>Likelihood</em>: During training, given some outcome we determine the likelihood of observing theta that maximizes the probability of that outcome (MLE).</li></ul><p>Bayes Theorem</p><ul><li>Revise prediction using new evidence</li><li><em>Naive Bayes Classifier</em>: Generative Classification model</li></ul><p>Linear Transformation
$f(\alpha x + \beta y)=\alpha f(x)+\beta f(y)$</p><ul><li>Linear transformations: rotation (produced by shearing), scaling</li></ul><p>Affine Transformation
$$f(\alpha x + (1-\alpha)y)=\alpha f(x)+(1-\alpha)f(y)$$
AKA a linear transformation plus translation
$$f(x)=\vec{a}^Tx+\vec{b}$$
<em>Properties Perserved:</em></p><ol><li>Collinearity between points: points on same line (collinear points) remain on same line after transformation</li><li>Parallelism: Parallel lines remain parallel</li><li>Convexity of sets: Furthermore, extreme points of original set map to extreme points of transformed set</li><li>Length Ratios of Parallel lines</li><li>Barycenters of weighted collections of points. Aka center of mass. <em>?</em></li></ol><p>Machine Learning
:PROPERTIES:
:heading: true
:collapsed: true
:END:</p><p>Discriminative Model</p><ul><li>learn decision boundaries between classes</li><li>SVM, Logistic Regression, Decision Trees</li><li>Not great w outliers</li><li><em>Maximizes conditional likelihood</em>, given model parameters<ul><li>L(theta) = max P(y | x; theta)</li></ul></li></ul><p>Generative Model</p><ul><li>distribution of classes themselves</li><li>Naïve Bayes, Discriminant Analysis (LDA, GDA)</li><li>Better with outliers</li><li><em>Maximizes joint likelihood:</em> the joint probability given model parameters<ul><li>L(theta)=max P(x, y; theta)</li></ul></li></ul><p>Cross Validation</p><ul><li>Typically use k-fold validation: i.e. leave one out cross validation</li><li><em>Roll forward Cross Validation</em>: used with Time Series Data</li></ul><p>Tree Pruning</p><ul><li>Mitigate Overfitting</li></ul><p>Cost Effective Pruning:</p><ul><li>Remove a subtree (replacing with a leaf node)</li><li>if resulting tree does not have a significant decrease in performance (delta formula) then keep the new pruned tree and repeat.</li></ul><p>Dealing with Datasets</p><p><em>Imbalanced Datasets</em></p><ul><li><em>Random Under-sampling</em>: Lots of data in smaller class</li><li><em>Random Over-sampling</em>: Not lots of data in smaller class</li></ul><p>Missing Data:</p><ul><li>Imputation (i.e. 0), add a new category for categorical (I.e. “other”), interpolation</li></ul><p>Outliers</p><ul><li>*Analyze without and without outliers</li><li>Trimming: Remove outliers</li><li>Winsorizing: Ceil/Floor to a max/min non-outlier value</li></ul><p>SMOTE</p><ul><li>Synthetic Monetary Oversampling*</li><li>Synthesize new data with minor noise added to existing sample rather than exact copies</li></ul><p>ROC
:PROPERTIES:
:id: 6304ff68-bb00-4200-aa44-975f5ef5aba9
:collapsed: true
:END:</p><ul><li>Receiver Operator Characteristic</li><li>Graphs Sensitivity vs Specificity (OR Precision)</li><li>i.e. True Positive vs True Negative Rates
[[../assets/unnamed_1661273383447_0.png]]</li></ul><p>Accuracy
True predictions/Number points</p><p>Precision
precision=TP/(TP+FP)</p><ul><li>How many of our positive predictions were right?</li><li>Positive Prediction Accuracy for the label</li><li>Proportion of positive results that were correctly classified</li><li>$\text{precision}=\text{true_pos}/(\text{true_pos} + \text{true_neg})$</li><li>Good if we have an imbalance such as way more negatives than positives (not in eq)</li></ul><p>Sensitivity/Recall
\begin{align}
\text{sensitivity}&=TP/(TP+FN)\
&=\text{true_pos}/(\text{true_pos} + \text{false_neg})
\end{align}</p><ul><li>Returns the True Positive Rate of the label.</li></ul><p>Specificity: TN/(TN+FP)
$$\text{specificity}=\text{false_pos}/(\text{false_pos} + \text{true_neg})$$</p><p>AUC</p><ul><li>Area Under the Curve</li><li>Used to compare ROC curves</li><li>More AUC=better</li></ul><p>Neural Networks</p><p>RNN</p><ul><li>Handle sequential data (unlike feedforward nn).</li><li>Sentiment analysis, text mining, image captioning, time series problems</li></ul><p>CNNs</p><ul><li>Image matrix, filter matrix</li><li>Slide filter matrix over the image acompute the dot product to get convolved feature matrix.</li><li><em>CNN better than Dense NN for Images:</em> Because less params (no overfit), more interpretable (can look at weights), CNNs can learn simple-to-complex patterns (learn complex patterns by learning several simple patterns)</li></ul><p>GANs</p><ul><li>Use a Generator and Discriminator (to build an accurate Discriminator model)</li></ul><p>Activation Functions</p><p>Softmax</p><ul><li>Scales input to (0,1). Output layers</li></ul><p>ReLU</p><ul><li>Clips input at 0, only non-negative outputs.</li><li>Produces “rectified feature map.” Hidden layers</li></ul><p>Swish</p><ul><li>Variant of ReLU developed at google, better at some DL tasks</li></ul><p>Pooling</p><ul><li>Pooling is a down-sampling operation that reduces the dimensionality of the feature map</li></ul><p>Computation Graph</p><ul><li>Nodes are operations, Edges are tensors/data</li></ul><p>Batch Gradient vs Stochastic Gradient Descent</p><p>Autoencoder
:PROPERTIES:
:collapsed: true
:END:</p><ul><li>3 Layer model that tries to reconstruct its input using a hidden layer of fewer dimensions to create a latent space representation.</li><li>In its most basic form, uses dimensionality reduction to perform filtering (i.e. noise).</li></ul><p>Regularized Autoencoders:
Classification (include Sparse, Denoising, Contractive)</p><p>Variational Autoencoders:
Generative models</p><p>Uses</p><ul><li>Extract features and hidden patterns</li><li>Dimensionality reduction</li><li>Denoise im ages</li><li>Convert black and w hite images into colored images.</li></ul><p>Transfer Learning</p><ul><li>Models: VGG-16, BERT, GPT-3, Inception, Xception</li></ul><p>Vanishing Gradients</p><ul><li>Use ReLU instead of tanh (try different activation function)</li><li>Try Xavier initialization (takes into account number of inputs and outputs).</li></ul><p>ANN Hyperparameters</p><ol><li><em>Batch size:</em> size of input data</li><li><em>Epochs:</em> number of times training data is visible to the neural network to train.</li><li><em>Momentum:</em> Dampen/attenuate oscillations in gradient descent. If the weights matrix is ill conditioned, this helps convergence speed up a bit.</li><li><em>Learning rate:</em> Represents the time required for the network to update the parameters and learn.</li></ol><p>Ensemble learning</p><ul><li>[[Boosting]], Bagging, Random Forest</li><li>Aggregation mitigates overfitting of a class</li></ul><p>Bagging</p><ul><li>Train several models and vote to produce output.</li></ul><p>Boosting</p><ul><li>Use a model to improve performance where another model is weakest. (i.e. model the error)</li></ul><p>Logistic Regression</p><ul><li>Regression for classification.</li><li>Linear model produces logits, softmax(logits) produces prediction</li></ul><p>Fourier Transform</p><ul><li>Decompose functions into its constituent parts.</li></ul><p>Quant
:PROPERTIES:
:heading: true
:id: 63050186-cb96-4e65-9445-a39fa91e2305
:collapsed: true
:END:</p><ul><li>What factors in production could cause a backtested strategy to perform different than expected?<ul><li>Slippage, transaction costs, systemic risk, outside events that cannot be modeled such as state of global economy/climate/legislation/etc</li></ul></li></ul><p>Black-Scholes</p><ul><li>Originally to valuate European call options</li><li>American equivalents: Bjerksund-Stendland model, binomial, trinomial models</li><li>Uses 5 Factors:<ol><li>Volatility</li><li>Price of underlying asset</li><li>Strike price</li><li>Time to expiration</li><li>Risk free interest rate</li></ol></li><li><em>Black-Scholes Asumptions</em><ul><li>Price follows a random walk approximately Geometric brownian motion with constant drift and volatility (i.e. log(variance) is constant)</li><li>No dividends over life of option</li><li>Movements are random, market is random</li><li>No transaction costs</li><li>RFR and volatility are constant (not a strong assumption for volatility, since that is influenced by supply/demand)</li><li>Returns are log normal</li><li>Option is European (can only be exercised at expiration)</li></ul></li></ul><p>Options</p><p>Option Greeks</p><p><em>Delta</em>
First derivative with respect to price. Rate of change of equilibrium price (aka BS price) with respect to asset price.</p><p><em>Gamma</em>
Second derivative with respect to price.</p><p><em>Theta</em>
First derivative with respect to time-to-maturity. Rate of change of equilibrium price with respect to time-to-maturity.</p><p><em>Vega</em>
Rate of change of equilibrium price with respect to asset volatility.</p><p><em>Rho</em>
Rate of change of equilibrium price with respect to RF interest rate.</p><p>Call Options</p><ul><li><em>Break-even</em>: K + P (where K is strike price and P is cost of option)</li><li><em>5 reasons to buy a call option</em><ol><li>Bet on upside move with minimal cost (lot of a exposure for little cost)</li><li>Unlimited Upside</li><li>Limited Downside: Can only lose what you paid for the option</li><li>Increase in Volatility: Option is priced based on its volatility, so all we need is an increase in volatility to increase the value of our option</li><li>Hedge Short Position: Unlimited upside offsets risk of short as shorts have unlimited downside</li></ol></li></ul><p>Call-Spread</p><ul><li>Max Value: difference in strike prices. $v_{max} = K_2 - K_1$<ul><li>Where $K_2=Sold$ and $K_1=bought$ strike prices</li></ul></li><li>Max Loss: $Loss_{max} = v_{max}-P_{cs}$<ul><li>Max value - Price of call-spread</li></ul></li></ul><p>Pay Off Diagrams</p><ul><li>Plot of <em>Underlying Price vs. P&L</em></li><li>3 Key Points:<ol><li>Maximum Loss</li><li>Maximum Gain</li><li>Break-even Point</li></ol></li></ul><p>Put-Call Parity</p><ul><li>Represents an arbitrage opportunity</li><li>$\text{call_price}+\text{present_value_discounted} = \text{put_price} + \text{spot_price}$<ul><li>(where present value is discounted from the value at RFR)</li></ul></li></ul><p>Finance Fundamentals
:PROPERTIES:
:heading: true
:id: 63050437-3d0c-448f-a280-7e3a230472ae
:collapsed: true
:END:</p><p><em>Financial Statements</em></p><p><em>Balance sheet</em>
Shows a company&rsquo;s&mldr;</p><ol><li>Assets</li><li>Liabilities</li><li>Shareholders&rsquo; equity (what it owns, owes, is worth)</li></ol><ul><li>Highlights: liquidity, capital assets, credit metrics, liquidity ratios, leverage, ROA (return on assets), ROW (return on equity)</li></ul><p><em>Income Statement</em>
Shows a company&rsquo;s&mldr;</p><ol><li>Revenue</li><li>Expenses</li><li>Net Income</li></ol><ul><li>Highlights: Growth rates, margins, profitability</li></ul><p><em>Cash Flow Statement</em>
Shows a company&rsquo;s cash inflows/outlflows from&mldr;</p><ol><li>Operation</li><li>Investments</li><li>Financing</li></ol><ul><li>Highlights: short/long term cash flow profile, needs to raise money or return capital to shareholders</li></ul><p><em>What is the best financial statement to measure a company&rsquo;s health?</em></p><ul><li>Cash is king. /Cash Flow Statement/ shows how much cash company is actually generating</li><li>Arguments for other statements:</li><li>Balance Sheet: assets are true driver of cash flow</li><li>Income Statement: Earning power and profitability on an accural basis</li></ul><p>WACC</p><ul><li>Weighted Average Cost of Capital</li><li>Blended cost of capital across all sources (common/preferred shares, debt)</li><li>(% debt vs total capital) x (1-effective tax rate)+(% equity vs capital) x (required return on equity) <em>check this</em></li></ul><p><em>What is cheaper: debt or equity?</em></p><ul><li>Debt: backed by collateral and paid off before equity</li><li>Debt is more liquid <em>?</em></li></ul><p>Finance Formulas
:PROPERTIES:
:heading: true
:id: 63050473-e880-4c2c-863a-4b18153ab78d
:collapsed: true
:END:</p><ul><li>/Revenue/ = Volume x Price</li><li>/Cost/ = Fixed Cost + Variable Cost</li><li>/Profit/ = Revenue - Cost</li><li>/Profitability or Profit Margin/ = Profit/Revenue</li><li>/ROI/ = Annual Profit / Principal Investment</li><li>/Breakeven or Payback Period/ = Principal / Annual Profit</li><li>/ROE/ = Profits / Shareholder Equity</li><li>/ROA/ = Profits / Total Assets</li></ul><p>Word Problems
:PROPERTIES:
:collapsed: true
:END:</p><ul><li><em>Given a random number generator which provides a random real value between 0 to 1, how can you estimate the value of pi?</em><ul><li>Monte Carlo integration (unit circle inside a square, ratio of points in circle versus points outside)</li></ul></li><li><em>Find the minimum number of socks I need to take out from a box of red and black socks to ensure that I have k pairs of socks.</em><ul><li>Use <em>Pigeon Hole Principle</em><ol><li>Pick up N socks (one of each color)</li><li>Next sock forms a pair</li></ol></li><li>Answer: <em>2k+N-1</em></li><li>Note: When coding ensure to check if K>total_pairs in list (pairs+=arr[i]/2)</li></ul></li><li><em>Can you minimize piecewise linear function without adding auxiliary variables?</em><ul><li>[[https://www.seas.ucla.edu/~vandenbe/ee236a/lectures/pwl.pdf][See this lecture]]</li><li>Firstly: is the function convex</li><li>Convex piecewise-linear (piecewise-affine is a more accurate term) can be expressed as:
$$f(x)=\max <em>{i=1, \ldots, m}\left(a</em>{i}^{T} x+b_{i}\right)$$</li><li>Problem becomes: $\min f(x)$</li><li>Therefore minimize each:<ul><li>$\min t$ subject to $a_{i}^{T} x+b_{i} \le t$ for $i=1,..,m$</li><li>Basically: <em>no</em></li></ul></li></ul></li></ul><p>Code Puzzles
:PROPERTIES:
:heading: true
:END:</p><p>Reverse a Linked List
:PROPERTIES:
:heading: true
:collapsed: true
:END:
#+begin_src python
Curr=head
while curr:
Next = curr.next
Curr.next = prev
Prev = curr
Curr = next
return prev (new head)
#+end_src</p><p>Longest Palindrome
:PROPERTIES:
:heading: true
:collapsed: true
:END:
#+begin_src python
def longest_palindrome(s: str):
if not s:
return “”
longest = “”
for i in range(len(s)):
# odd case, like “aba”
tmp = helper(s, i, i)
if len(tmp) > len(longest):
# update result
longest = tmp</p><pre><code>     # even case, like “abba”
     tmp = helper(s, i, i+1)
     if len(tmp) &gt; len(longest):
          longest = tmp
</code></pre><p>return longest</p><p>def helper(s: str, l: int, r: int):
while l >= 0 and r &lt; len(s) and s[l] == s[r]:
l -= 1 #decrement the left
r += 1 #increment the right
return s[l+1:r]</p><p>#+end_src</p><p>BFS (not recursive)
:PROPERTIES:
:heading: true
:collapsed: true
:END:
#+begin_src python</p><h1 id=visit-adjacent-unvisited-vertex>Visit adjacent unvisited vertex.<a hidden class=anchor aria-hidden=true href=#visit-adjacent-unvisited-vertex>#</a></h1><h1 id=--mark-it-as-visited-display-it-insert-it-in-a-queue>- Mark it as visited. Display it. Insert it in a queue.<a hidden class=anchor aria-hidden=true href=#--mark-it-as-visited-display-it-insert-it-in-a-queue>#</a></h1><h1 id=if-no-adjacent-vertex-pop-vertex-off-queue>If no adjacent vertex, pop vertex off queue<a hidden class=anchor aria-hidden=true href=#if-no-adjacent-vertex-pop-vertex-off-queue>#</a></h1><h1 id=repeat-rule-1-and-rule-2-until-the-queue-is-empty>Repeat Rule 1 and Rule 2 until the queue is empty.<a hidden class=anchor aria-hidden=true href=#repeat-rule-1-and-rule-2-until-the-queue-is-empty>#</a></h1><p>def bfs(graph, current_node):
visited = []
queue = [current_node]</p><pre><code>while queue:
    s = queue.pop(0)
    print(s)
    for neighbor in graph[s]:
        if neighbor not in visited:
            visited.append(neighbor)
            queue.append(neighbor)
</code></pre><p>bfs(graph, &lsquo;A&rsquo;)
#+end_src</p><p>DFS (not recursive)
:PROPERTIES:
:heading: true
:collapsed: true
:END:
#+begin_src python</p><h1 id=add-unvisited-nodes-to-stack>add unvisited nodes to stack<a hidden class=anchor aria-hidden=true href=#add-unvisited-nodes-to-stack>#</a></h1><p>def dfs(graph, start_vertex):
visited = set()
traversal = []
stack = [start_vertex]
while stack:
vertex = stack.pop()
if vertex not in visited:
visited.add(vertex)
traversal.append(vertex)
stack.extend(reversed(graph[vertex])) # add in same order as visited
return traversal</p><p>#+end_src</p><p>Check if binary trees are equal:
:PROPERTIES:
:heading: true
:END:
#+begin_src python
def are_identical(root1, root2):
if root1 == None and root2 == None:
return True</p><p>if root1 != None and root2 != None:
return (root1.data == root2.data and
are_identical(root1.left, root2.left) and
are_identical(root1.right, root2.right))</p><p>return False
#+end_src</p><p>Etc
:PROPERTIES:
:heading: true
:END:
#+begin_src python
#+end_src</p></div><hr><aside><h4>No linked references</h4></aside><br><aside class=related><h3>Similar Content</h3><ul><p class=capitalize><a style=color:var(--link) href=/pages/machine-learning/>Machine Learning</a></p></ul></aside><footer class=post-footer><ul class=post-tags><li><a href=https://datatician.io/tags/career/>career</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://datatician.io>Datatician</a></span><br><span>Powered by
<a href=https://github.com/sawhney17/logseq-schrodinger rel=noopener target=_blank>Logseq Schrödinger</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>